{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "# Ignore the warning message\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(y_test, x_test, X_train, y_train, cv, X, y):\n",
    "    \"\"\"\n",
    "    method fits classifier, carries out cross validation and calculates performance metrics\n",
    "    optional code to print tree as a figure or text\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=2)\n",
    "    knn = knn.fit(X_train, y_train)\n",
    "\n",
    "    # calculate scores with cv\n",
    "    scores_a = cross_val_score(\n",
    "        knn, X_train, y_train, scoring=\"accuracy\", cv=cv, n_jobs=-1\n",
    "    )\n",
    "    a = mean(scores_a)\n",
    "\n",
    "    scores_p = cross_val_score(\n",
    "        knn, X_train, y_train, scoring=\"precision\", cv=cv, n_jobs=-1\n",
    "    )\n",
    "    p = mean(scores_p)\n",
    "\n",
    "    scores_r = cross_val_score(\n",
    "        knn, X_train, y_train, scoring=\"recall\", cv=cv, n_jobs=-1\n",
    "    )\n",
    "    r = mean(scores_r)\n",
    "\n",
    "    scores_f1 = cross_val_score(knn, X_train, y_train, scoring=\"f1\", cv=cv, n_jobs=-1)\n",
    "    f1_cv = mean(scores_f1)\n",
    "\n",
    "    # evaluate the model on the test set\n",
    "    pred = knn.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    y_pred = knn.predict_proba(x_test)[:, 1]\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # predict and calculate probability\n",
    "    pred = knn.predict(x_test)\n",
    "    probs = knn.predict_proba(x_test)\n",
    "\n",
    "    # perform permutation importance\n",
    "    results = permutation_importance(knn, X, y, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc, probs, knn, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify data frames and set range and feature_cols\n",
    "\n",
    "metrics_KNN = pd.DataFrame(\n",
    "    columns=[\"Sentence\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]\n",
    ")\n",
    "featureImportance = pd.DataFrame(columns=[\"Sentence\", \"Feature\", \"Score\"])\n",
    "\n",
    "n = list(range(2, 61))\n",
    "\n",
    "feature_cols = [\n",
    "    \"Erstloesung\",\n",
    "    \"Schussel\",\n",
    "    \"Erfolg\",\n",
    "    \"Schwierigkeit\",\n",
    "    \"ist_Schulzeit\",\n",
    "    \"MehrfachFalsch\",\n",
    "    \"vorher_abgebrochen\",\n",
    "    \"Fehler\",\n",
    "    \"Klassenstufe\",\n",
    "    \"Jahredabei\",\n",
    "    \"AnzahlAufgaben\",\n",
    "    \"Sex__m\",\n",
    "    \"Sex__w\",\n",
    "    \"Testposition__pruefung\",\n",
    "    \"Testposition__training\",\n",
    "    \"Testposition__version\",\n",
    "    \"Art__GK\",\n",
    "    \"Art__GR\",\n",
    "    \"Art__GZ\",\n",
    "    \"Art__K\",\n",
    "    \"Art__LB\",\n",
    "    \"UserAttribut\",\n",
    "    \"OrderNumber\",\n",
    "    \"steps\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops through all matrices and fits model, saves metrics\n",
    "\n",
    "for i in n:\n",
    "    path = \"matrices_allsessions/matrix\" + str(i) + \".pkl\"\n",
    "    infile = open(path, \"rb\")\n",
    "    df = pickle.load(infile)\n",
    "    infile.close()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y = df.y\n",
    "    y = y.astype(\"int\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=1\n",
    "    )\n",
    "    k = 5\n",
    "    cv = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "    a, p, r, f1, auc, probs, knn, results = KNN(\n",
    "        y_test, X_test, X_train, y_train, cv, X, y\n",
    "    )\n",
    "    metrics_KNN = metrics_KNN.append(\n",
    "        {\n",
    "            \"Sentence\": i,\n",
    "            \"Accuracy\": a,\n",
    "            \"Precision\": p,\n",
    "            \"Recall\": r,\n",
    "            \"F1\": f1,\n",
    "            \"AUC\": auc,\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # summarize feature importance\n",
    "    importance = results.importances_mean\n",
    "    for k, v in enumerate(importance):\n",
    "        # print(\"Feature: %0d, Score: %.5f\" % (k, v))\n",
    "        featureImportance = featureImportance.append(\n",
    "            {\"Sentence\": i, \"Feature\": k, \"Score\": v}, ignore_index=True\n",
    "        )\n",
    "\n",
    "metrics_KNN.to_pickle(\"model_metrics/metrics_KNN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "featureImportance_grouped = featureImportance.groupby(\"Feature\").agg(\n",
    "    {\"Score\": [\"mean\"]}\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "ax = sns.barplot(\n",
    "    x=featureImportance_grouped.index,\n",
    "    y=featureImportance_grouped.Score[\"mean\"],\n",
    "    data=featureImportance_grouped,\n",
    "    color=\"#00338d\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "sns.set_theme()\n",
    "\n",
    "sns.lineplot(data=metrics_KNN, x=\"Sentence\", y=\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=metrics_KNN, x=\"Sentence\", y=\"Precision\")\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=metrics_KNN, x=\"Sentence\", y=\"Recall\")\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=metrics_KNN, x=\"Sentence\", y=\"F1\")\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=metrics_KNN, x=\"Sentence\", y=\"AUC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate sample probability data\n",
    "# probability_sample = probs[:, :1].tolist()\n",
    "# probability_sample = pd.DataFrame(probability_sample)\n",
    "# probability_sample.to_pickle('KNN55.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
